{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the PeakCompare wiki","text":"<p>These pages give an overview of the pipeline given in PeakCompare and how to interpret the output metric of said pipeline..</p> <p>For installation instructions, please see the repository's README file</p>"},{"location":"#recommended-order-of-reading","title":"Recommended order of reading","text":"<p>If you are new to PeakCompare, it is recommended that you read this documentation in the order that reflects the steps of the pipeline:</p> <p>1) Peak call 2) Peak compare 3) Metric interpretation</p>"},{"location":"#navigating-these-pages","title":"Navigating these pages","text":"<p>There are a few keyboard shortcuts you can use on this site for navigation:</p> <ul> <li><code>s</code> , <code>f</code> , <code>/</code> -&gt; Opens up the search bar (search all pages)</li> <li><code>n</code> , <code>.</code> -&gt; Goes to the next page</li> <li><code>p</code> , <code>,</code> -&gt; Goes to the previous page</li> </ul>"},{"location":"metric_interpretation/","title":"Metric interpretation","text":"<p>For a formal definition of the metric, head over to  this page.</p> <p>A value close to one implies that the region in the comparison dataset is a comparable peak as to what is seen in the reference dataset. This is stronger than \"the peak is called in both data sets in this region\". This is because the metric takes into account the p-values that are used to call the peaks by MACS. This metric will only be close to 1 if both datasets have similar levels of significance (or the comparison dataset has even more significant evidence for a peak).</p> <p>A value close to zero implies that the region in the comparison dataset is not a comparable peak as to what is seen in the reference dataset. If you were to use MACS only, you may well find that the region may have peaks called in the comparison dataset. However, here we require that the significance of said peak is comparable in each dataset. In this sense the metric is more strict than simply looking at overlaps between the datasets.</p> <p>The metric in reality can vary between zero and one and it is down to the user (and their downstream analyses) of what they want to do with this information. You may choose to simply use the metric in a binary sense, where a certain threshold means the peak is in both datasets. Alternatively you might want to apply this methodology to get the metric for a wide range of regions or a wide set of samples. Once obtained, correlation analysis could be used to see how variable peaks are in certain regions of the genome for certain samples/cell types.</p>"},{"location":"metric_interpretation/#note","title":"Note","text":"<p>In the event that the comparison set has more significant evidence for having peaks than the reference set, it is advised to switch the roles of the two datasets (and rerun analysis).</p> <p>You may also encounter scenarios where the metric exceeds 1. This is likely to be the case if the region selected does not house a peak in the reference dataset. In such scenarios it is recommended to either switch the roles of the datasets or to inspect a different region of the genome.</p>"},{"location":"peak_calling/","title":"Peak Calling","text":"<p>Before running the pipeline that is used to calculate the metric, a few different files are required that can be obtained via MACS. Usually, peak calling with MACS is via the function <code>callpeaks</code>. However, this process does not give us the outputs of intermediate steps that MACS carries out. As such the script <code>PeakCall.sh</code> will use the fundamental functions of MACS to generate these intermediate files:</p> <ul> <li>The coverage/pileup track</li> <li>The bias track</li> <li>The p-values track</li> <li>The set of peaks before merge process</li> <li>The set of peaks after merge process</li> </ul> <p>If you want to understand the steps behind the MACS peak calling process, it is recommended that you read the  official tutorial.</p>"},{"location":"peak_calling/#running","title":"Running","text":"<p>To run this script, you will need to complete the  setup first. After this, you can submit the script <code>PeakCall.sh</code> to a SLURM queue with:</p> <pre><code>sbatch -p queue .../PeakCall.sh path/to/config_file.txt\n</code></pre> <p>The configuration file to use for this script is given in <code>example_config_call.txt</code>. The queue to send the job to will depend on the HPC system you are using. You can view all available partitions with:</p> <pre><code>sinfo show partitions | awk '{print $1}' | uniq\n</code></pre> <p>The script is designed to work with or without a configuration file and will process a single set of reads in any format MACS can work with. It is expected that you run this script for each sample (or merged set of samples) so it is recommended that you have two configuration files (so jobs can run concurrently).</p>"},{"location":"peak_calling/#why-merged-and-unmerged-peaks","title":"Why merged and unmerged peaks","text":"<p>In the final step of the peak calling process, MACS encourages the user to  merge any peaks that are suitably close together. The recommended minimum gap between peaks is the read length of the dataset. The metric needs to be able to differentiate between peaks that are called due to this merging process because such base pair positions will have very low coverage. The metric heavily relies on comparing p-values and read counts, if a peak has very low p-values and read counts, this can be misleading if the merging process is not accounted for.</p>"},{"location":"peak_calling/#cutoff-analysis","title":"Cutoff analysis","text":"<p>In order to call peaks, a line in the sand must be drawn somewhere. This often feels arbitrary and is a difficult topic in statistics. However, MACS does allow for cutoff analysis where different thresholds are used and some metrics are displayed for each threshold used. These metrics are:</p> <ul> <li>The number of peaks</li> <li>The average length of the peaks</li> <li>The total length of all peaks combined</li> </ul> <p>Using these there are a couple of different approaches you can take to get a suitable value for the cutoff. </p> <ul> <li>Elbow analysis</li> <li>Using biological knowledge</li> <li>The expected number of peaks</li> <li>The expected average peak length</li> </ul> <p>Currently the pipeline allows you to either pick a hard cutoff value or use the most stringent cutoff that gives you a certain average peak length. If you have your own idea here, a file is put in your chosen output directory under <code>(sample_name)_cutoff_analysis.txt</code>. The pipeline doesn't use elbow analysis as this is hard to automate (\"How do you define a dramatic change?\").</p>"},{"location":"peak_calling/#time","title":"Time","text":"<p>The time MACS takes to peak call will vary with the number of reads that you have, however the process is usually very fast. For a dataset with 11,000,000 reads the total time taken was roughly 5 minutes  (where the cpu was Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz).</p>"},{"location":"peak_comparing/","title":"Peak Comparing","text":"<p>After completing the peak calling pipeline for both your reference dataset and your comparison dataset, you can now compare regions between the two. The purpose of this script is to generate a metric that aims to tell the user how likely that a comparable peak in the comparison dataset exists (given a peak exists in the reference dataset). Details on how to interpret the output metric can be found here.</p>"},{"location":"peak_comparing/#running","title":"Running","text":"<p>Before running this script, it is assumed you have ran the peak calling  pipeline for both of your datasets. After accomplishing this, identify a peak (or selection of close by peaks) in the reference dataset (noting down the  chromosome, start and end base positions). Feed this information into the configuration file <code>example_config_compare.txt</code>. This configuration file asks for a few different parameters, head over to [this section] to understand what these parameters mean and how they are used.</p> <p>After filling out the configuration file, submit the job to a SLURM queue with</p> <pre><code>sbatch -p queue .../PeakCompare.sh path/to/config_file.txt\n</code></pre>"},{"location":"peak_comparing/#ignoring-this-script","title":"Ignoring this script","text":"<p>This script is only given to provide a means for the user to submit the job into a SLURM queue. You could alternatively just run the underlying python script yourself. To do this, activate the conda environment with:</p> <pre><code>conda activate PeakCompare-MACS\n</code></pre> <p>Then run the python script called <code>peak_compare.py</code> which can be found in the <code>Python_Scripts</code> directory of this repository. There are quite a few arguments that this script requires due to the multitude of files it needs to read in (another reason behind the wrapper script). For this reason, you may decide to create your own wrapper script that achieves your own goals (perhaps you want to look at multiple regions in the genome simultaneously).</p>"},{"location":"peak_comparing/#grep","title":"grep","text":"<p>To save time on reading in files, the wrapper script uses grep to pull out the chromosome of interest before passing them to <code>peak_compare.py</code>. If you plan on running the python script interactively (or you are writing your own wrapper script), consider implementing this as well.</p>"},{"location":"peak_comparing/#how-the-metric-is-calculated","title":"How the metric is calculated","text":"<p>The metric is a simple ratio of the number of bases in psuedopeaks in the comparison and the number of bases in peaks in the reference dataset. Pseudopeaks is a term that describes regions in your comparison set that are 'almost' peaks when compared to the reference dataset. A more formal definition is given here.</p> <p>As alluded to in cutoff analysis, cutoffs are tricky little things. No matter what cutoff you choose, you will almost always be classing some regions as peaks because they only just exceed the cutoff (and classing some regions not as peaks for the opposite reason). This is problematic when comparing two datasets. How can we determine if a peak is truly absent in the comparison dataset when there is a possibility that a single extra read in the region could tip the balance? </p> <p>The solution given with this tool is to look at the confidence intervals for each p-value instead of looking at whether they hit certain thresholds. If the confidence intervals for each dataset overlap, it suggests that (under the chosen significance) the true value could be shared by each dataset at this point. Considering these intervals is both more lax and strict than simply using a single threshold value. If the region in the reference dataset is a very significant peak (very low p-value), then this method required the same region in the comparison dataset to have similarly small p-values to be considered as a pseudopeak.</p> <p>Comparing confidence intervals is useful, but comes with a drawback in this particular case of using MACS to determine peaks. As mentioned in  this section, some peaks may have a very high p-value (not significant), and yet be called a peak due to the merging process. This is why both files are required by this python script and allows us to now formally state what defines a psuedopeak.</p>"},{"location":"peak_comparing/#pseudopeak-definition","title":"Pseudopeak definition","text":"<p>Def<sup>n</sup>: A base position is defined to be in a pseudopeak region if it satisfies either of these criteria:</p> <p>(i) The base position is within a peak region in the unmerged reference dataset peaks, and the confidence interval in the comparison dataset overlaps or exceeds the reference dataset's confidence interval (for this base position).</p> <p>(ii) The base position surpasses the cutoff threshold in the comparison dataset and is only within a peak region in the merged reference dataset peaks (not present in the unmerged peaks).</p>"},{"location":"peak_comparing/#confidence-interval-calculation","title":"Confidence interval calculation","text":"<p>It should be noted first that p-values from MACS are calculated using the Poisson distribution (as pileup counts generally follow this distribution). Now, the confidence interval calculation is carried out in two steps:</p> <p>1) A confidence interval is calculated for each position in the bias track. This requires the following steps:   - The local variance (see window size) for the bias track is   calculated.   - The local standard error is calculated.   - The confidence interval is found by using \\(\\hat\\lambda = \\lambda \\pm   z_\\alpha \\cdot std\\_err\\). Where \\(z_\\alpha\\) is the critical value for the   given significance. 2) A confidence interval for the p-value is calculated for each position using the Poisson distribution with arguments:   - The \\(\\hat\\lambda\\) confidence interval calculated in 1)   - The number of reads in that position seen in the coverage (pileup) track</p>"},{"location":"peak_comparing/#parameters","title":"Parameters","text":"<p>The underlying python script requires a few parameters that can be specified in the configuration file. These are:</p> <ul> <li>The cutoff</li> <li>The significance</li> <li>The window size</li> <li>'unmerged'</li> </ul>"},{"location":"peak_comparing/#cutoff","title":"Cutoff","text":"<p>This is the cutoff that was used with the reference dataset when peak calling. If you didn't set this value yourself (or simply forgot), you can find out what cutoff value was used by looking at the log file for the job you submitted. This should look like:</p> <pre><code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       \u2502 File: 01-Jan~00-12345678.log\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   1   \u2502 Using a cutoff value of 5.\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre>"},{"location":"peak_comparing/#significance","title":"Significance","text":"<p>This is the significance used when calculating the confidence intervals described here. Picking a value that is smaller will result in tighter confidence intervals and therefore results in stricter criteria for psuedopeaks. There is no recommended value here, just be aware that confidence intervals are being compared against each other (so a significance of 95% is more like 90%).</p>"},{"location":"peak_comparing/#window-size","title":"Window size","text":"<p>In calculating the confidence intervals used to determine pseudopeaks, a window is used to calculate local variance in the bias track. A suitable value for the window size will be between a quarter and half of the fragment length for each dataset. It should be noted that a smaller window size will result in the variance calculated being more localised. However, this comes with the caveat that less variance can be observed (consider the case when window size is 0). A careful balance should be chosen here.</p>"},{"location":"peak_comparing/#unmerged","title":"Unmerged","text":"<p>You may wish to discount peak regions in the reference dataset that are a result of the merging process entirely when calculating the metric. To do this you can specify <code>UNMERGED=1</code> in the configuration file (or use <code>--unmerged</code> if using the python script). This will mean that the metric will only look at the ratio of psuedopeaks (in the comparison dataset) with the unmerged peaks (in the reference dataset). All peaks that are a result of merging will be ignored.</p>"},{"location":"peak_comparing/#time","title":"Time","text":"<p>Running this script is fast (a couple of seconds). The main slowdown comes with reading in files. This is why grep is utilised in the provided wrapper script. The complexity of your data can also increase read times as bedgraph files (output of most MACS commands) can massively vary in size due to this factor. The bedgraph file at minimum can be 23 lines long (one for each chromosome) up to ~2,700,000,000 lines long (one for each base pair).</p>"}]}